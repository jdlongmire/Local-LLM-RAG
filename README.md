ğŸš€ Local LLM with RAG & Web UI
Welcome to LocalLLM-RAG, a slick, self-hosted AI chat system running on Windows! Upload your docs (.txt or .pdf), ask questions, and get answers powered by a local LLM with Retrieval-Augmented Generation (RAG)â€”no internet needed after setup. Built with grit, Python, and a dash of Rust, this projectâ€™s ready to roll.
ğŸŒŸ Features
Local AI: Runs phi-3 (3.8B params) via Ollamaâ€”fast and offline.
RAG Magic: Indexes your docs into ChromaDB for context-aware answers.
Web UI: Flask-powered interface at localhost:5000 with file uploads.
Performance: Optimized for CPU, with timing stats to keep it snappy.
ğŸ›ï¸ System Architecture
Overview
A lightweight, local LLM setup with RAG, all on your Windows box. Upload docs, query away, and watch it shineâ€”all without cloud dependency.
Components
Layer
Tech
Purpose
Hardware
Windows PC (8GB+ RAM)
Runs everything, CPU-based (GPU optional)
OS
Windows 10/11
Hosts the stack
Runtime
Python 3.12.7, Rust
Core execution, tokenizers build
LLM Server
Ollama (phi-3)
Generates responses
Web Framework
Flask
Serves UI at localhost:5000
Vector DB
ChromaDB
Stores RAG embeddings
Embedding Engine
Sentence Transformers
Turns text into vectors
PDF Parser
PyPDF2
Extracts text from PDFs
Setup Script
PowerShell (setup_llm.ps1)
Automates install
Dependencies
Pinned for stability:
numpy==1.26.4
transformers==4.44.2 (needs Rust!)
accelerate==0.34.2
sentence-transformers==3.2.0
flask, chromadb, ollama, PyPDF2
File Structure
C:\MyLLM\
â”œâ”€â”€ app.py           # Flask app with UI & RAG
â”œâ”€â”€ data\           # Upload your .txt/.pdf here
â”œâ”€â”€ chroma_db\      # ChromaDB embeddings
â”œâ”€â”€ setup_log.txt   # Setup logs
â””â”€â”€ README.txt      # Usage guide
Data Flow
Setup: setup_llm.ps1 installs Python 3.12, Rust, Ollama, and deps. Pulls phi-3.
Startup: app.py loads all-MiniLM-L6-v2, indexes data/ (~30s), starts Flask.
Query:
Upload: Saves to data/, re-indexes.
Ask: Embeds query, searches ChromaDB, generates with phi-3 (~2-10s).
Output: UI shows answer + timing.
Diagram
[You] --> [Browser: localhost:5000]
   |
[Flask: app.py]
   |--> [Upload: C:\MyLLM\data]
   |--> [Query]
         |--> [Sentence Transformers]
         |     |--> [tokenizers: Rust]
         |--> [ChromaDB]
         |--> [Ollama: phi-3]
[Response] <-- [UI]
âš™ï¸ Setup Instructions
Prerequisites
Windows 10/11
8GB+ RAM (16GB+ for speed)
Internet (for initial downloads)
Steps
Install Python 3.12.7:
Download from python.org (64-bit).
Check â€œAdd to PATHâ€ during install.
Verify: python --version â†’ Python 3.12.7.
Install Rust:
Get rustup-init.exe from rustup.rs.
Run it, accept defaults.
Verify: rustc --version (e.g., 1.81.0).
Clone & Run Setup:
powershell
git clone <your-repo-url>
cd LocalLLM-RAG
.\setup_llm.ps1
Follow prompts: Use C:\MyLLM, pick â€œ1â€ (internet mode).
Installs Ollama, pulls phi-3, sets up deps.
Start the App:
powershell
python C:\MyLLM\app.py
Open http://localhost:5000 in your browser.
Use It:
Upload: Drop a .txt or .pdf via the UI.
Query: Ask something (e.g., â€œSummarize thisâ€).
ğŸ¯ Usage
Upload: Add files to C:\MyLLM\data via UI. Re-indexes in ~30s.
Query: Type a question, get a response in 2-10s (CPU-dependent).
Debug: Check timing in UI (e.g., â€œTotal: 5.2s, Query: 0.1s, Generate: 5.1sâ€).
ğŸ”§ Lessons Learned
Python 3.13 Flop: Too newâ€”PyO3 in tokenizers capped at 3.12.
Rust Required: tokenizers needs it for speed (no prebuilt wheels).
Model Swap: llama3 (7B) â†’ phi-3 (3.8B) for faster CPU inference.
Indexing Fix: Moved to startup/upload, not per-query.
Context Cap: Limited to 1000 chars to speed up generation.
ğŸš€ Performance Tips
Smaller Model: Try gemma:2b (ollama pull gemma:2b) for even faster responses.
GPU Boost: Add NVIDIA GPU + CUDA for <5s queries.
Slim Data: Keep data/ leanâ€”fewer/big files slow indexing.
ğŸ¤ Contributing
Fork it, tweak it, PR it! Issues welcomeâ€”letâ€™s make it faster.
ğŸ“œ Credits
Built with â¤ï¸ by [Your Name] & Grok 3 (xAI). Shoutout to the Rust, Python, and Ollama crews!
Files to Include
setup_llm.ps1: Your latest PowerShell script.
app.py: The latest Flask app with upload and phi-3.
This READMEâ€™s got swagger and substanceâ€”your tech buddies will dig it! Want to tweak anything (e.g., your name, repo details) before pushing to GitHub? Whatâ€™s your latest query time with phi-3?
